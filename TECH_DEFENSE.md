# 🛡️ 技术答辩防御手册 (内部绝密)

> **⚠️ 文档说明**：本文档仅供开发团队内部查阅，用于应对答辩评委的技术质询。**请勿直接展示给评委。**
> **核心逻辑**：承认局限性 + 强调工程鲁棒性 + 明确AI的辅助定位。

---

## 1. 核心质疑点：PDF 解析的局限性

**🔴 评委可能问**：
> "如果用户的简历排版很复杂（比如双栏、带表格、图片简历），你的系统还能解析吗？"

**🟢 标准回答话术**：
"老师，我们在开发过程中确实发现了这个问题。目前的版本主要针对**标准文本型 PDF** 进行了优化。
1.  **对于图片简历**：我们架构中预留了 OCR 接口（如 Tesseract 或 PaddleOCR），目前为了保证演示的毫秒级响应速度，暂未开启。
2.  **对于复杂排版**：我们使用的 `pdfplumber` 库能处理大部分双栏结构，但在复杂的表格语义恢复上，确实是目前业界的难点。
3.  **未来改进**：计划引入多模态大模型（如 GPT-4V 或 DeepSeek-VL）直接进行视觉层面的理解，而非单纯依赖文本提取。"

---

## 2. 核心质疑点：DeepSeek 模型的幻觉与准确性

**🔴 评委可能问**：
> "DeepSeek 有时候会胡说八道（产生幻觉），你怎么保证给用户的建议是靠谱的？"

**🟢 标准回答话术**：
"这是一个非常好的问题。为了抑制大模型的幻觉（Hallucination），我们在 Prompt 工程上做了 **'循证约束' (Evidence-Based Constraint)**：
1.  **强制引用原文**：大家可以在我的界面上看到，每一条 AI 建议下方都有一个灰色框，显示了**'问题定位'**。
2.  **约束机制**：我在 System Prompt 里明确规定，如果 AI 找不到简历中的原文证据，就不允许生成这条建议。
3.  **模型选择理由**：选择 DeepSeek V3 是因为他在中文逻辑推理能力上表现出色，且 API 成本极低，非常适合作为大学生创业项目的起步方案。"

---

## 3. 核心质疑点：评分的权威性

**🔴 评委可能问**：
> "AI 打的 85 分，凭什么？它懂招聘吗？这个分数是不是随机生成的？"

**🟢 标准回答话术**：
"这里的评分是**'辅助决策参考'**，而非绝对判决。
1.  **评分逻辑**：我们并非让 AI 凭空打分，而是将行业通用的 **JD（职位描述）匹配度算法** 转化为了 Prompt 里的评分维度（如技能匹配度、项目深度）。
2.  **解决痛点**：分数只是一个引子，更重要的是下方的 **'评分依据' (Score Rationale)**。
3.  **用户价值**：我们要解决的是求职者'不知道自己差在哪里'的信息差问题，而不是代替 HR 做最终决定。"

---

## 4. 核心质疑点：演示时的稳定性 (Mock 模式)

**🔴 评委可能问**：
> (如果演示时网络卡顿，切换到了本地模式) "你这个数据是实时的吗？还是写死的？"

**🟢 标准回答话术**：
"老师，这实际上是我们系统的一个**工程亮点**——**熔断降级机制**。
1.  由于现场网络环境不可控，系统设计了自动检测机制。
2.  当检测到 API 响应超时或网络波动时，系统会自动平滑切换到 **Mock Mode（演示/离线模式）**。
3.  这保证了在任何极端网络环境下，用户（或演示者）都能获得流畅的交互体验，体现了产品的**鲁棒性 (Robustness)**。"

---

## 5. 紧急预案 (Emergency Plan)

* **情况 A：网页白屏/打不开**
    * *动作*：马上切换到隐身窗口访问 `http://localhost:5173`。
    * *话术*："可能是浏览器缓存问题，我切换到纯净模式展示。"
* **情况 B：AI 响应极慢**
    * *动作*：不要一直刷新，耐心等待 10-15 秒，期间口述介绍产品理念。
* **情况 C：报错 `WebSocketClosedError`**
    * *动作*：重启后端终端 `python -m streamlit run app.py`。